Use the logit_bias in order to bias it towards better jsons and also could possibly bias it towards picking words it has already chosen before.

I am essentially using Solo performance prompting (SPP) which just means that I am using multiple personas to complete the task. Right now it is always the same task and always in the same order so I have the system messages and function order predetermined and everything. HOWEVER, for more variable tasks it could eventually come up with those prompts and personas itself and call those functions itself. Chatgpt does have the functionality to say it needs to call a function. Could make a higher up AI which just decides which high level functions to call, each high level function could then have some other AI which determines which lower level function to call and so on. Essentially having the decision making with AI and then you outline what the AI can do. can build better chatbot by having the orignial prompt just decide which chatbot should answer it. then have specialized chatbots for each which have specialized prompts and parameters. could even maybe have AI dynamically change that prompt and those parameters to improve it.

could make a chatbot which makes this type of chatbot. Where the initial prompt just has as input the possible topics or questions it can get asked. determine which personas it must make, make each persona, and improve each persona. 

more likely to be able to only work for turning unstructured data into structured data

Ran into rate limit problems. Was because would continue to try when the error was just too many tokens instead of switching to 16k

somehow got an endless loop of ands where the llm would simply output indented ands over and over again. honestly don't know why.

some clincial trials just have notes in the middle of it, make sure to exclude these notes from criteria

a lot of them will have clarifications or descriptions between parentheses, these are the real annoying and specific problems






Since we couldn't meet here is a quick overview of what I've been doing. For starters I realized that while langchain is really nice to get started, its really just a wrapper around GPT-3 and has some limitations. So I've been working on directly calling OpenAI's API using their python client. This allows me to customize things for what I need in order to break down the task as much as possible. I am going to try to enforce even more the idea of code just providing the infrastrucutre and then having the AI fill in the details. By that I mean I am going to break the AI task down into as many steps as possible with very specific prompts for each step. Then I will build the code to call each step in order and combine the results. Part of the reason is that even when I manually made my own mql from text to provide and example for each edge case it would still forget some specific rules. especially with the big ones. Some of these clinical trials have so much text and criteria that it has forgotten or put less importance to specific instructions. Instead what I want to do is have it focus on one small piece at a time. The first step already takes care of the boolean algebra seperation and splits it into individual parts. Then I will have it go through each part one by one and translate it to mql. This way it can't forget anything and has to fully understand each piece before moving on. I then just combine the results. However, the mql also has different types of sentences to convert. I have quite a couple rules about numbers, but if the specific sentence doesn't use any numbers (e.g: must have lung cancer) then all of those number rules are just making the rules which do pertain to it less important. So what I want to do is have a prompt which decides what type of sentence it is, whether its numerical, categorical, true or false, and then within those numerical can be seperated into timeframe and quantity and other ways of breaking it down further. (of course I don't need AI to determine wether it is numerical or not, but I do need it for other more subjective types of breaking it down). Then I can only apply the rules which pertain to it and it should provide a much much more accurate result, albeit at more cost. 

I honestly think this type of thinking could be expanded and generalized for more than just this clinical trial and even automated more. Imagine an AI agent where its first rule is to recursively break a task down into smaller tasks untill it reaches a certain AI atomic unit (this could be a potential abstract point). at that point it starts making the prompts for that task and then goes back up and combining the smaller tasks to form the bigger ones untill it formulates an answer. It is essentially a hopefully improved version of Solo performance prompting (SPP) technique. SPP is similar except it is all in the same prompt and is just taking different personalities, For example if I need to write something scientific then one persona is a writer and another is a scientist. But it would just say act like a ___ and then provide input. The difference is this method that I am thinking of is not just for personas but also on an individual task. For example it would split up a scientific paper into its different components like the abstract, introduction, evidence, conclusion and so on; then it would break each of those down into further parts and write prompts for each specific part. This comes at a cost of much more llm calls, But I think will really allow you to get a very specific output. just like what we need for this or any changing of unstructured to structured data. This could also potentially be very helpful for structuring doctor and nurses notes on a patient, and now you have a database you can easily search through. for example you need to quickly figure out when the last time this patient did xyz, if it was written in the notes then now its structuered and super easy to find. for that honestly just vectorizing the data would be enough for a regular llm call by itself to figure it out and give you the response. However when you need to do analysis on data in a database, then that is where this strategy comes in handy. You can also use the vectorizing of data to force it use specific words I believe, solving the problem of phrasing things differently. Essentially every change gets vectorized based on its input. and then when you go to structurize a new data it searches the vector database for any other previous structurizations that might be similar based on its input and gets told to use the same phrasing for its output. Essentially I think I know how to solve both problems now and am much more confident that they will work, I just need to implement it.

I have also done more things behind the scenes like running into rate limit problems and fixing that but the most important find of the week is whats above. I do need input however on one thing. Would you rather me dive deep into the llm and prompt engineering aspect to really get that accurate structuring of data, or would you rather have me use this already functioning but not perfect mql that I can get and finish the other parts of the project (frequency analysis and asking questions to get data). I think since the point of this is more on the llm side we should focus on that but let me know which path you want me to take.