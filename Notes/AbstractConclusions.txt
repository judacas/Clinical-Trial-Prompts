Breaking a prompt down into smaller pieces has been the biggest improvement to prompts.

Just how much splitting up is too much? can we define an atomic unit for AI. So far, I saw the problem was too big and split it up into 3 parts. Boolean representation, MQL, and verifying that it works as a JSON representation. Can I split up each part even more? for example can I split up each avoid and prefer into its own prompt? have the program recursively prompt it with each avoid and prefer? This would be many times more costly, but could it convert them more accurately? 

Could I prompt engineer a prompt which determines the individual steps of a task and makes prompts for each task?

Providing specific examples is necessary, however they must encompass all rules. For example, if one of the specific avoid prefers rules is not explicitly shown in the example, it is much less likely to properly work

Could I prompt engineer a chatbot which determines whether or not it has enough information to do its task? For example, if it is given a new type of phrase where none of the avoid prefers actually to pertain to it. Could it determine that it does not have enough information to do its task? prompt the user on how it would solve it with a given example and then add that new rule to its prompt? would allow for much quicker process of edge cases instead of having to realize it does it wrong every once in a while, and seeing why.

Can most things be demonstrated in rules with avoid prefer examples?

Does everything that I have said only pertain to translating/changing representations of text?

Can this be applied to a general idea of turning unstructured data into structured data?

Can AI be used to make its own prompts better and recursively improve itself? So far I did that with GPT-4 and asking it how it understood the prompt and what was unclear. This allowed me to be more specific in certain parts and clarify what should happen when this or that happens. Could I make this an automated process where it improves itself or does need a human which knows the end goal to nudge it along?

Could dynamically changing the logit_bias help it make more structured and homogenized data? Instead of trying the method before where it finds two different representations of the same thing and tries to make them the same, could it just dynamically change the logit_bias to rewrite them as the same?
