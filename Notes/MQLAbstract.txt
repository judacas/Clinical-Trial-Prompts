Background
Matching cancer patients to clinical trials represents an important mechanism for empowering patients and expanding clinical research to a broader population. Minority accrual to clinical trials in the US is not representative of the overall population, and an improved clinical trial matchmaking service may be a solution. Clinical trials are designed with specific eligibility criteria; however, two major barriers for screening patient eligibility against clinical trial inclusion/exclusion criteria are a generally-accepted formal representation of the eligibility criteria and the ability to convert free-text criteria into that formal representation (1,2). We evaluated the possibility of using LLMs for converting free-text clinical trial eligibility into structured eligibility criteria.

Methods
OpenAI’s ChatGPT (gpt-3.5-turbo-0613) was used (gpt-3.5-turbo-16k-0613 for large descriptions) via the OpenAI API with Python (v3.11.4) bindings. The ClinicalTrials.gov REST API 2.0.0-draft was used to fetch clinical trials. A temperature of 0 was used for all prompts except for fixing JSONs, for which the temperature value increased 1 for each try. Ten interventional cancer clinical trials were chosen for training. Responses were evaluated for correctness, and prompts were modified in an iterative development process (3). Prompts were decomposed into more specific subtasks, and examples were manually derived. The output format was the MongoDB Query Language (MQL). Specific prompts, and code, are available in our GitHub repository (https://github.com/judacas/Clinical-Trial-Prompts/blob/main/Code/prompts.py).

Results
Our goal was to develop ChatGPT prompts to translate trial eligibility text into a structured data format (MQL). Translation directly from free text to MQL was inefficient and frequently failed to separate criteria into discrete rules (e.g., multiple individual criteria “A and B or C and D” were not split). Problem decomposition was used to break prompts into multiple smaller components following the single responsibility principle. This drastically improved the effectiveness of the responses. Three AI prompts were ultimately derived. The first prompt splits free text into individual criteria with Boolean operators joining individual criterion (Boolean representation). The second prompt converts the Boolean representation into MQL through the use of Avoid/Prefer statements such as ‘AVOID: age : Adult’ and ‘PREFER: age: {$gt: 18, $lt: 65}’. The third prompt fixes any syntactical errors that go against the MQL and JSON standards. Due to the token context limitation of 16k in our models, not all clinical trials could be considered in the LLM simultaneously. An iterative approach for homogenizing data was used by storing each criterion in a list so that subsequent MQL conversions cross-referenced this list. This reduced the total number of predicates by merging overlapping concepts (e.g., ECOG status and ECOG medical status).

We qualitatively evaluated the results of these trials with a data set (n=5) of therapeutic clinical trials not used for the prompt engineering training. We evaluated the number of predicates in the inclusion and exclusion criteria from ClinicalTrials.gov versus the predicates identified as discrete elements in the MQL representation. The system identified all inclusion criteria in 4 trials and missed 4 predicates from one trial, due to the OpenAI API limitation in number of tokens (16k) per request. In total, 101 out of 105 (96%) predicates were included in MQL. Due to the same limitation, the system did not classify the exclusion criteria in 2 trials. In the remaining 3, our implementation identified 100% of the predicates in the exclusion criteria (39 in total).

Conclusion
We evaluated the feasibility of using LLMs to translate free-text clinical trial eligibility criteria into a structured format (MQL). Despite technical limitations, the LLM was able to appropriately identify most predicates from the evaluated trials. We identified several key considerations when designing the prompts for translation. First, translation worked best independently by trial followed by a standardization process. Secondly, problem decomposition is an essential aspect of utilizing LLMs. Rather than relying on the model to accurately perform all tasks at once, we found the models more accurate using the single responsibility principle for prompt engineering. By separating tasks into identifying inclusion/exclusion predicates and then managing the translation to a structured format, we provide data supporting the feasibility of the approach.