it does not have any knowledge on clinical trials, thus the NCT number does not do anything


specify the number of lists at all times. otherwise it may make an inclusion and exclusion list sperately.

repeat information, for example don't say repeat the previous proccess, instead retype the proccess. This also goes with repeating specific requirements, for example in making the must and must nots I specify to not lose any specific numbers or details. however in changinge it to questions if I don't restate that all details must remain then some details will be lost.

New threads for new clincal trials is better

will we have enough token memory???????

what if instead of changing musts and must nots directly into questions, we first merge the statements into one list, then make questions from that? Should help avoid the inherent non determinsitic translation from making it seem like two statements are no longer similar. 

Something that goes hand in hand with that is change the must and must not statements to more of a condition style format. for example lets say one must have an ecog reading of 2 or less and another must have an ecog reading of more than 2. GPT might see that and think they are two different questions. but in reality it can be one question with different "correct" responses. so next I will try is instead of having musts and must nots how about I make it list the things GPT will need to know about the user without including the correct answer. for example instead of saying must be 18+ it will say must know age. instead of saying must not have brain metastasis it will say ask if they have brain metastasis. this can then make some sort of database of things to know. we will then make a seperate list for each question which has all of the different conditions.
for example one thing that most trials will need to know is the age. We then have a list of requirements for that question. for example <18, >18, >50, etc. we then have a list of clincial trials which pertain to each condition.

This will create a sort of tree structure where instead of the clincial trial being at the upper most level, they are the leaves of the tree. This should also help with the token limits and token memory.

Just joining two lists (currently questions) is already 1,896 tokens. that very roughly approximates to only being able to combine up to 4 or 16 lists depending on either 8k or 32k version. Corection, that is only the input. if you include the output then combining 2 lists is already 3,330 tokens. This is unaceptable.

Keep in mind there is also token memory, which means making more lists won't remember what the old lists are.




meeting notes:
path seems to be ctml determines what we need to know from the user, chatgpt asks the user and acquires information and explains any questions they may have. chatgpt takes that information and parses it into a json or ctml or some other format which we can then use match miner to find the corresponding clinical trial. chatgpt does not do the matchmaking proccess.