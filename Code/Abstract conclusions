breaking a prompt down into smaller pieces has been the biggest improvement to prompts.

just how much splitting up is too much? can we define an atomic unit for AI. So far I saw the problem was too big and split it up into 3 parts. boolean representation, MQL, and verifying that it works as a json. Can I split up each part even more? for example can I split up each avoid and prefer into its own prompt? have the program recusively prompt it with each avoid and prefer? This would be many times more costly, but could it convert them more accurately? could I prompt engineer a prompt which determines the individual steps of a task and makes prompts for each task?

providing specific examples is neccesary, however they must encompass all rules. for example if one of the specific avoid prefers rules is not explicilty shown in the example, it is much less likely to properly work

Could I prompt engineer a chatbot which determines wether or not it has enough information to do its task? for example if it is given a new type of phrase where none of the avoid prefers actually pertain to it. could it determine that it does not have enough information to do its task? prompt the user on how it would solve it with a given example and then add that new rule to its prompt? would allow for much quicker proccess of edge cases instead of having to realize it does it wrong every once in a while and seeing why.

can most things be demonstrated in rules with avoid prefer examples?

does everything that I have said only pertain to translating/changing representations of text?

can this be applied to a general idea of turning unstructured data into structuered data?

can ai be used to make its own prompts better and recursively improve itself? so far I did that with gpt-4 and asking it how it understood the prompt and what was unclear. this allowed me to be more specific in certain parts and clarify what should happen when this or that happens. could I make this an automated proccess where it improves itself or does need a human which knows the endgoal to nudge it along?

could dynamically changing the loit_bias help it make more structured and homogenized data? instead of trying the method before where it finds two different representations of the same thing and tries to make them the same, could it just dynamically change the loit_bias to rewrite them as the same?
