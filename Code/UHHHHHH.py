# Load environment variables

from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)
from langchain.llms import OpenAI
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())

# Run basic query with OpenAI wrapper

llm = OpenAI(model_name="text-davinci-003")
llm("explain large language models in one sentence")

# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4


chat = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3)
messages = [
    SystemMessage(content="You are an expert data scientist"),
    HumanMessage(
        content="Write a Python script that trains a neural network on simulated data ")
]
response = chat(messages)

print(response.content, end='\n')

# # Import prompt and define PromptTemplate

# from langchain import PromptTemplate

# template = """
# You are an expert data scientist with an expertise in building deep learning models.
# Explain the concept of {concept} in a couple of lines
# """

# prompt = PromptTemplate(
#     input_variables=["concept"],
#     template=template,
# )

# # Run LLM with PromptTemplate

# llm(prompt.format(concept="autoencoder"))

# # Import LLMChain and define chain with language model and prompt as arguments.

# from langchain.chains import LLMChain
# chain = LLMChain(llm=llm, prompt=prompt)

# # Run the chain only specifying the input variable.
# print(chain.run("autoencoder"))

# # Define a second prompt

# second_prompt = PromptTemplate(
#     input_variables=["ml_concept"],
#     template="Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words",
# )
# chain_two = LLMChain(llm=llm, prompt=second_prompt)

# # Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input

# from langchain.chains import SimpleSequentialChain
# overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)

# # Run the chain specifying only the input variable for the first chain.
# explanation = overall_chain.run("autoencoder")
# print(explanation)

# # Import utility for splitting up texts and split up the explanation given above into document chunks

# from langchain.text_splitter import RecursiveCharacterTextSplitter

# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size = 100,
#     chunk_overlap  = 0,
# )

# texts = text_splitter.create_documents([explanation])

# # Individual text chunks can be accessed with "page_content"

# texts[0].page_content

# # Import and instantiate OpenAI embeddings

# from langchain.embeddings import OpenAIEmbeddings

# embeddings = OpenAIEmbeddings(model_name="ada")

# # Turn the first text chunk into a vector with the embedding

# query_result = embeddings.embed_query(texts[0].page_content)
# print(query_result)

# # Import and initialize Pinecone client

# import os
# import pinecone
# from langchain.vectorstores import Pinecone


# pinecone.init(
#     api_key=os.getenv('PINECONE_API_KEY'),
#     environment=os.getenv('PINECONE_ENV')
# )

# # Upload vectors to Pinecone

# index_name = "langchain-quickstart"
# search = Pinecone.from_documents(texts, embeddings, index_name=index_name)

# # Do a simple vector similarity search

# query = "What is magical about an autoencoder?"
# result = search.similarity_search(query)

# print(result)

# # Import Python REPL tool and instantiate Python agent

# from langchain.agents.agent_toolkits import create_python_agent
# from langchain.tools.python.tool import PythonREPLTool
# from langchain.python import PythonREPL
# from langchain.llms.openai import OpenAI

# agent_executor = create_python_agent(
#     llm=OpenAI(temperature=0, max_tokens=1000),
#     tool=PythonREPLTool(),
#     verbose=True
# )

# # Execute the Python agent

# agent_executor.run("Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1")
